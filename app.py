
###################################
#
# ç¬¬1æ­¥ï¼šé…ç½®æœ¬åœ°æ¨¡å‹
#
###################################

from llama_index.core import Settings

# é…ç½®ollamaçš„LLMæ¨¡å‹ï¼Œè¿™é‡Œæˆ‘ä»¬ç”¨gemma:2b
from llama_index.llms.ollama import Ollama
llm_ollama = Ollama(model="mistral-nemo:12b-instruct-2407-q8_0", request_timeout=600.0)

from llama_index.embeddings.ollama import OllamaEmbedding
embed_model= OllamaEmbedding("mistral-nemo:latest")


# é…ç½®Rerankæ¨¡å‹ï¼Œè¿™é‡Œæˆ‘ä»¬ç”¨BAAI/bge-reranker-base
# from llama_index.core.postprocessor import SentenceTransformerRerank
# rerank_model_bge_base = SentenceTransformerRerank(
# model="D:\ollama\embed_model\bge-reranker-base", 
# top_n=2
# )

# é…ç½®ä½¿ç”¨SpacyTextSplitter
from llama_index.core.text_splitter import CodeSplitter
Settings.llm = llm_ollama
Settings.embed_model = embed_model
Settings.text_splitter = CodeSplitter(
        language="cpp",
        chunk_lines = 2000,
        chunk_lines_overlap = 1024,
        max_chars = 5120
    )

###################################
#
# ç¬¬2æ­¥ï¼šé…ç½®å‘é‡æ•°æ®åº“
#
###################################

import chromadb
from llama_index.core import StorageContext
from llama_index.vector_stores.chroma import ChromaVectorStore

STORAGE_DIR = "./storage"# å®šä¹‰ç´¢å¼•ä¿å­˜çš„ç›®å½•

db = chromadb.PersistentClient(path=STORAGE_DIR)
chroma_collection = db.get_or_create_collection("think")
chroma_vector_store = ChromaVectorStore(chroma_collection=chroma_collection)
chroma_storage_context = StorageContext.from_defaults(vector_store=chroma_vector_store)

###################################
#
# ç¬¬3æ­¥ï¼šåˆå§‹åŒ–Streamlit Webåº”ç”¨
#
###################################

import streamlit as st

st.set_page_config(page_title="æœ¬åœ°å¤§æ¨¡å‹çŸ¥è¯†åº“RAGåº”ç”¨", page_icon="?", layout="centered", initial_sidebar_state="auto", menu_items=None)
st.title("æœ¬åœ°å¤§æ¨¡å‹çŸ¥è¯†åº“RAGåº”ç”¨")
st.info("By ree", icon="ğŸ‘¤")  # ä½¿ç”¨ç”¨æˆ·å›¾æ ‡

if "messages" not in st.session_state.keys(): # åˆå§‹åŒ–èŠå¤©å†å²è®°å½•
    st.session_state.messages = [
       {"role": "assistant", "content": "å…³äºæ–‡æ¡£é‡Œçš„å†…å®¹ï¼Œè¯·éšä¾¿é—®"}
    ]

###################################
#
# ç¬¬4æ­¥ï¼šåŠ è½½æ–‡æ¡£å»ºç«‹ç´¢å¼•
#
###################################

from llama_index.core import VectorStoreIndex, SimpleDirectoryReader

DATA_DIR = r"D:\sql\openGauss-server\src\gausskernel\storage\access\redo"# çŸ¥è¯†åº“æ–‡æ¡£æ‰€åœ¨ç›®å½•

@st.cache_resource(show_spinner=False)
def load_data():
    with st.spinner(text="åŠ è½½æ–‡æ¡£å¹¶å»ºç«‹ç´¢å¼•ï¼Œéœ€è¦1-2åˆ†é’Ÿ"):
# å°†æŒ‡å®šç›®å½•ä¸‹çš„æ–‡æ¡£å»ºç«‹ç´¢å¼•ï¼Œä¿å­˜åˆ°å‘é‡æ•°æ®åº“
        documents = SimpleDirectoryReader(input_dir=DATA_DIR, recursive=True, exclude=["*.txt", "Makefile"]).load_data() 
        index = VectorStoreIndex.from_documents(
            documents, storage_context=chroma_storage_context)
    return index

def load_index():
# ç›´æ¥ä»å‘é‡æ•°æ®åº“è¯»å–ç´¢å¼•
    index = VectorStoreIndex.from_vector_store(chroma_vector_store)
    return index

###################################
#
# ç¬¬5æ­¥ï¼šå®šåˆ¶Promptæ¨¡æ¿
#
###################################

from llama_index.core import PromptTemplate

text_qa_template_str = (
"ä»¥ä¸‹ä¸ºä¸Šä¸‹æ–‡ä¿¡æ¯\n"
"---------------------\n"
"{context_str}\n"
"---------------------\n"
"è¯·æ ¹æ®ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”æˆ‘çš„é—®é¢˜æˆ–å›å¤æˆ‘çš„æŒ‡ä»¤ã€‚å‰é¢çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å¯èƒ½æœ‰ç”¨ï¼Œä¹Ÿå¯èƒ½æ²¡ç”¨ï¼Œä½ éœ€è¦ä»æˆ‘ç»™å‡ºçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ä¸­é€‰å‡ºä¸æˆ‘çš„é—®é¢˜æœ€ç›¸å…³çš„é‚£äº›ï¼Œæ¥ä¸ºä½ çš„å›ç­”æä¾›ä¾æ®ã€‚å›ç­”ä¸€å®šè¦å¿ äºåŸæ–‡ï¼Œç®€æ´ä½†ä¸ä¸¢ä¿¡æ¯ï¼Œä¸è¦èƒ¡ä¹±ç¼–é€ ã€‚æˆ‘çš„é—®é¢˜æˆ–æŒ‡ä»¤æ˜¯ä»€ä¹ˆè¯­ç§ï¼Œä½ å°±ç”¨ä»€ä¹ˆè¯­ç§å›å¤ã€‚\n"
"é—®é¢˜ï¼š{query_str}\n"
"ä½ çš„å›å¤ï¼š "
)

text_qa_template = PromptTemplate(text_qa_template_str)

refine_template_str = (
"è¿™æ˜¯åŸæœ¬çš„é—®é¢˜ï¼š {query_str}\n"
"æˆ‘ä»¬å·²ç»æä¾›äº†å›ç­”: {existing_answer}\n"
"ç°åœ¨æˆ‘ä»¬æœ‰æœºä¼šæ”¹è¿›è¿™ä¸ªå›ç­” "
"ä½¿ç”¨ä»¥ä¸‹æ›´å¤šä¸Šä¸‹æ–‡ï¼ˆä»…å½“éœ€è¦ç”¨æ—¶ï¼‰\n"
"------------\n"
"{context_msg}\n"
"------------\n"
"æ ¹æ®æ–°çš„ä¸Šä¸‹æ–‡, è¯·æ”¹è¿›åŸæ¥çš„å›ç­”ã€‚"
"å¦‚æœæ–°çš„ä¸Šä¸‹æ–‡æ²¡æœ‰ç”¨, ç›´æ¥è¿”å›åŸæœ¬çš„å›ç­”ã€‚\n"
"æ”¹è¿›çš„å›ç­”: "
)
refine_template = PromptTemplate(refine_template_str)

###################################
#
# ç¬¬6æ­¥ï¼šåˆ›å»ºæ£€ç´¢å¼•æ“ï¼Œå¹¶æé—®
#
###################################

# ä»…ç¬¬ä¸€æ¬¡è¿è¡Œæ—¶ä½¿ç”¨load_dataå»ºç«‹ç´¢å¼•ï¼Œå†æ¬¡è¿è¡Œä½¿ç”¨load_indexè¯»å–ç´¢å¼•
# index = load_data();
index = load_index();
st.write("Debug:  print session keys 1", st.session_state.keys())

# åˆå§‹åŒ–æ£€ç´¢å¼•æ“
if "query_engine" not in st.session_state.keys():
    st.write("Debug:  print session keys 2", st.session_state.keys())
    query_engine = index.as_query_engine(
    text_qa_template=text_qa_template, 
    refine_template=refine_template,
    similarity_top_k=6, 
    response_mode="compact", 
    verbose=True)
    st.write("Debug: query engine is", query_engine)
    st.session_state.query_engine = query_engine

# æç¤ºç”¨æˆ·è¾“å…¥é—®é¢˜ï¼Œå¹¶å°†é—®é¢˜æ·»åŠ åˆ°æ¶ˆæ¯å†å²è®°å½•
if prompt := st.chat_input("Your question"):
    st.session_state.messages.append({"role": "user", "content": prompt})

# æ˜¾ç¤ºæ­¤å‰çš„é—®ç­”è®°å½•
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])

# ç”Ÿæˆå›ç­”
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = st.session_state.query_engine.query(prompt)
            st.write(response.response)
            message = {"role": "assistant", "content": response.response}
            st.session_state.messages.append(message)