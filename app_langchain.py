import logging
import streamlit as st
from langchain.chains import RetrievalQA
from langchain_core.prompts import PromptTemplate
from langchain_community.llms import Ollama
from langchain.callbacks.manager import CallbackManager
from typing import List, Dict, Any
from langchain_core.callbacks import BaseCallbackHandler
from langchain_chroma import Chroma
import sys
import os
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import Language,RecursiveCharacterTextSplitter
from langchain_community.document_loaders.generic import GenericLoader
from langchain_community.document_loaders.parsers import LanguageParser
class RAGDebugHandler(BaseCallbackHandler):
    def __init__(self):
        self.retriever_inputs = []
        self.retriever_outputs = []
        self.llm_inputs = []
        self.llm_outputs = []
        self.current_response = ""
        # self.placeholder = st.empty()

    def on_retriever_start(self, query: str, **kwargs: Any) -> None:
        logging.info(f"Retriever query: {query}")
        self.retriever_inputs.append(query)

    def on_retriever_end(self, documents: List[Any], **kwargs: Any) -> None:
        logging.info(f"Retrieved {len(documents)} documents")
        for i, doc in enumerate(documents):
            logging.info(f"Document {i + 1}:\n{doc.page_content[:200]}...")
        self.retriever_outputs.extend(documents)

    def on_llm_start(self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any) -> None:
        for i, prompt in enumerate(prompts):
            logging.info(f"LLM Prompt {i + 1}:\n{prompt}")
        self.llm_inputs.extend(prompts)

    def on_llm_end(self, response: Any, **kwargs: Any) -> None:
        # logging.info(f"LLM Response:\n{response}")
        self.llm_outputs.append(response)

    # def on_llm_new_token(self, token: str, **kwargs: Any) -> None:
    #     """Run on new LLM token. Only available when streaming is enabled."""
    #     st.write(token, end="")
embed_model_code = HuggingFaceEmbeddings(
    model_name="./jina-embeddings-v2-base-code",
    model_kwargs = {
                    "device": "cpu",
                    "trust_remote_code": True
                },
    encode_kwargs = {"normalize_embeddings": True}
)

def indexing_vec():
    # load the code and split it into chunks
    # repo_path = r"D:\sql\openGauss-server\src\gausskernel\storage\access\redo"jnnjnjjjjn
    repo_path = r"/home/code/sql/openGauss-server/src/gausskernel/storage/access/redo"
    loader = GenericLoader.from_filesystem(
        repo_path,
        glob="**/*",
        suffixes=[".cpp"],
        parser=LanguageParser(language=Language.CPP),
    )
    data = loader.load()
    print(len(data))
    contents = [document.page_content for document in data]

    # ç”¨åˆ†éš”ç¬¦ '\n\n--8<--\n\n' è¿žæŽ¥æ‰€æœ‰æ–‡æ¡£å†…å®¹
    combined_contents = "\n\n--8<--\n\n".join(contents)

    # å°†è¿žæŽ¥åŽçš„å†…å®¹å†™å…¥æ–‡ä»¶
    with open('output.txt', 'w', encoding='utf-8') as file:
        file.write(combined_contents)
    cpp_splitter = RecursiveCharacterTextSplitter.from_language(language=Language.CPP, chunk_size=3500, chunk_overlap=200)
    all_splits = cpp_splitter.split_documents(data)
    print(len(all_splits))
    vectorstore = Chroma.from_documents(documents=all_splits, embedding=embed_model_code, persist_directory="./chroma_db")
    return vectorstore
def load_vec():
    vectorstore = Chroma(persist_directory="./chroma_db", embedding_function=embed_model_code)
    return vectorstore

def get_vectorstore(flag=False):
    persist_directory = "./chroma_db"
    
    # æ£€æŸ¥æŒä¹…åŒ–æ–‡ä»¶å¤¹æ˜¯å¦éžç©º
    is_non_empty_directory = os.path.isdir(persist_directory) and bool(os.listdir(persist_directory))
    
    if flag or is_non_empty_directory:
        return load_vec()
    else:
        return indexing_vec()

vectorstore = get_vectorstore()
debug_handler = RAGDebugHandler()

def main():
    st.set_page_config(page_title="æœ¬åœ°å¤§æ¨¡åž‹çŸ¥è¯†åº“RAGåº”ç”¨", page_icon="?", layout="centered", initial_sidebar_state="auto", menu_items=None)
    st.title("æœ¬åœ°å¤§æ¨¡åž‹çŸ¥è¯†åº“RAGåº”ç”¨")
    st.info("By ree", icon="ðŸ‘¤")  # ä½¿ç”¨ç”¨æˆ·å›¾æ ‡

    if "messages" not in st.session_state.keys(): # åˆå§‹åŒ–èŠå¤©åŽ†å²è®°å½•
       st.session_state.messages = [
            {"role": "assistant", "content": "å…³äºŽæ–‡æ¡£é‡Œçš„å†…å®¹ï¼Œè¯·éšä¾¿é—®"}
    ]
    # Prompt
    template = """ä½¿ç”¨ä»¥ä¸‹ä¸Šä¸‹æ–‡æ¥å›žç­”æˆ‘æœ€åŽæå‡ºçš„é—®é¢˜ã€‚
    å¦‚æžœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±ç›´æŽ¥è¯´ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚
    è¯·åŸºäºŽæä¾›çš„ä»£ç ç‰‡æ®µå›žç­”é—®é¢˜ï¼Œé‡ç‚¹å…³æ³¨ä»£ç çš„åŠŸèƒ½ã€ç»“æž„å’Œå…³é”®é€»è¾‘ã€‚
    åŠ¡å¿…ä½¿ç”¨ä¸­æ–‡å›žç­”
    {context}
    é—®é¢˜: åŸºäºŽä»¥ä¸Šç»™å‡ºçš„ä»£ç ç‰‡æ®µï¼Œ{question}ï¼Œ ä½¿ç”¨ä¸­æ–‡å›žç­”"""
    QA_CHAIN_PROMPT = PromptTemplate(
        input_variables=["context", "question"],
        template=template,
    )
    # åˆå§‹åŒ–æ£€ç´¢å¼•æ“Ž)
    llm = Ollama(
        model="mistral-nemo:12b-instruct-2407-q8_0", 
        callback_manager=CallbackManager([debug_handler]), 
        base_url="http://host.docker.internal:11434"
    )

    retreval = vectorstore.as_retriever(
        search_type="mmr",
        search_kwargs={
            "k": 10,
            "fetch_k": 50,  # åˆå§‹æ£€ç´¢æ›´å¤šæ–‡æ¡£ï¼Œç„¶åŽä»Žä¸­é€‰æ‹©æœ€ç»ˆçš„ 10 ä¸ª
            "lambda_mult": 0.7  # åœ¨ç›¸å…³æ€§å’Œå¤šæ ·æ€§ä¹‹é—´å–å¾—å¹³è¡¡
        }
    )
    qa_chain = RetrievalQA.from_chain_type(
        llm,
        retriever = retreval,
        chain_type_kwargs={"prompt": QA_CHAIN_PROMPT},
        return_source_documents=True,  # è¿™å°†è¿”å›žæºæ–‡æ¡£
        callbacks=[debug_handler]
    )

    # æç¤ºç”¨æˆ·è¾“å…¥é—®é¢˜ï¼Œå¹¶å°†é—®é¢˜æ·»åŠ åˆ°æ¶ˆæ¯åŽ†å²è®°å½•
    if prompt := st.chat_input("Your question"):
        st.session_state.messages.append({"role": "user", "content": prompt})

    # æ˜¾ç¤ºæ­¤å‰çš„é—®ç­”è®°å½•
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.write(message["content"])

    # ç”Ÿæˆå›žç­”
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = qa_chain({"query": prompt})
                st.write(response["result"])
                message = {"role": "assistant", "content": response["result"]}
                st.session_state.messages.append(message)

if __name__ == "__main__":
    main()
